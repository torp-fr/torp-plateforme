import { supabase } from "./core/supabaseClient.js";
import {
  extractDocumentText,
  detectSourceType,
} from "./extractors/extractionService.js";
import { cleanText, removeHeaders, normalizeLineEndings } from "./processors/cleanText.js";
import { structureSections } from "./processors/structureSections.js";
import { smartChunkText } from "./processors/smartChunker.js";
import { generateBatchEmbeddings } from "./core/embeddingService.js";

const BATCH_SIZE = 50;
const CHUNK_SIZE = 1000;
const EMBEDDING_DIMENSION = 1536;
const POLL_INTERVAL = 10000;

async function downloadFile(storagePath) {
  try {
    const { data, error } = await supabase.storage
      .from("knowledge-files")
      .download(storagePath);

    if (error) {
      throw new Error(`Storage download failed: ${error.message}`);
    }

    return await data.arrayBuffer();
  } catch (error) {
    throw new Error(`Failed to download file: ${error.message}`);
  }
}

function fallbackChunking(text, chunkSize) {
  const chunks = [];
  const trimmedText = text.trim();

  if (!trimmedText || trimmedText.length === 0) {
    return chunks;
  }

  for (let i = 0; i < trimmedText.length; i += chunkSize) {
    chunks.push({
      content: trimmedText.substring(i, i + chunkSize),
      chunk_index: chunks.length,
      section_title: null,
      section_level: null,
      metadata: {},
    });
  }

  return chunks;
}

async function processDocument(doc) {
  const documentId = doc.id;
  console.log(`Processing: ${documentId}`);

  try {
    // Step 1: Claim document (atomic pattern)
    const { data: claimed, error: claimError } = await supabase
      .from("knowledge_documents")
      .update({
        ingestion_status: "processing",
        ingestion_progress: 10,
      })
      .eq("id", documentId)
      .eq("ingestion_status", "pending")
      .select()
      .single();

    if (claimError || !claimed) {
      console.log(`  ‚ö†Ô∏è Document already processing or invalid`);
      return;
    }

    // Step 2: Delete existing chunks to avoid duplicates
    console.log(`  üóëÔ∏è Cleaning up existing chunks...`);
    await supabase
      .from("knowledge_chunks")
      .delete()
      .eq("document_id", documentId);

    // Step 3: Download file
    console.log(`  üì• Downloading file...`);
    const arrayBuffer = await downloadFile(doc.file_path);

    // Step 4: Extract text based on file type
    console.log(`  üîç Extracting text...`);
    const extractionResult = await extractDocumentText(
      arrayBuffer,
      doc.file_path,
      doc.mime_type
    );
    const rawText = extractionResult.text;
    const sourceType = detectSourceType(doc.mime_type, doc.file_path);
    const extractionConfidence = extractionResult.confidence;

    console.log(
      `  ‚úÖ Extracted ${rawText.length} characters (${extractionConfidence})`
    );

    // Step 5: Clean and normalize text
    console.log(`  üßπ Cleaning text...`);
    let cleanedText = normalizeLineEndings(rawText);
    cleanedText = removeHeaders(cleanedText);
    cleanedText = cleanText(cleanedText);

    if (!cleanedText || cleanedText.trim().length === 0) {
      throw new Error("No text content to process after cleaning");
    }

    // Step 6: Structure text into sections
    console.log(`  üìö Structuring sections...`);
    const sections = structureSections(cleanedText);
    console.log(`  ‚úÖ Found ${sections.length} sections`);

    // Step 7: Create smart chunks with metadata
    console.log(`  ‚úÇÔ∏è Creating smart chunks...`);
    let chunks = smartChunkText(cleanedText, sections, CHUNK_SIZE);

    if (chunks.length === 0) {
      console.log(`  ‚ö†Ô∏è Smart chunker returned 0 chunks, activating fallback chunking`);
      chunks = fallbackChunking(cleanedText, CHUNK_SIZE);

      if (chunks.length === 0) {
        throw new Error(
          "Fallback chunking also returned 0 chunks - text too small or invalid"
        );
      }

      console.log(`  ‚úÖ Fallback created ${chunks.length} chunks`);
    } else {
      console.log(`  ‚úÖ Created ${chunks.length} chunks`);
    }

    // Step 8: Update progress
    await supabase
      .from("knowledge_documents")
      .update({
        ingestion_progress: 30,
      })
      .eq("id", documentId);

    // Step 9: Batch insert chunks
    console.log(`  üìù Inserting chunks (batch size: ${BATCH_SIZE})...`);
    let insertedChunks = 0;

    for (let i = 0; i < chunks.length; i += BATCH_SIZE) {
      const batch = chunks.slice(i, i + BATCH_SIZE);
      const payload = batch.map((chunk) => ({
        document_id: documentId,
        chunk_index: chunk.chunk_index,
        content: chunk.content,
        token_count: Math.ceil(chunk.content.length / 4),
        section_title: chunk.section_title,
        section_level: chunk.section_level,
        metadata: JSON.stringify(chunk.metadata),
        source_type: sourceType,
        extraction_confidence: extractionConfidence,
      }));

      const { error: insertError } = await supabase
        .from("knowledge_chunks")
        .insert(payload);

      if (insertError) {
        throw new Error(
          `Chunk insert failed at index ${i}: ${insertError.message}`
        );
      }

      insertedChunks += payload.length;
    }

    console.log(`  ‚úÖ Inserted ${insertedChunks} chunks`);

    // Step 10: Update progress before embeddings
    await supabase
      .from("knowledge_documents")
      .update({
        ingestion_progress: 50,
      })
      .eq("id", documentId);

    // Step 11: Generate batch embeddings
    console.log(`  ü§ñ Generating embeddings (batch mode)...`);
    const texts = chunks.map((c) => c.content);
    const embeddings = await generateBatchEmbeddings(texts);

    // Validate all embeddings
    for (let i = 0; i < embeddings.length; i++) {
      if (embeddings[i].length !== EMBEDDING_DIMENSION) {
        throw new Error(
          `Embedding ${i} has invalid dimension: ${embeddings[i].length}`
        );
      }
    }

    // Step 12: Batch update chunks with embeddings
    console.log(`  üìä Batch updating ${embeddings.length} embeddings...`);
    const now = new Date().toISOString();
    const updates = chunks.map((chunk, index) => ({
      document_id: documentId,
      chunk_index: chunk.chunk_index,
      embedding: embeddings[index],
      embedding_generated_at: now,
    }));

    const { error: upsertError } = await supabase
      .from("knowledge_chunks")
      .upsert(updates, { onConflict: ["document_id", "chunk_index"] });

    if (upsertError) {
      throw new Error(`Batch embedding update failed: ${upsertError.message}`);
    }

    console.log(`  ‚úÖ Updated ${embeddings.length} embeddings`);

    // Step 13: Mark as completed
    console.log(`  ‚úÖ Marking document as completed...`);
    const { error: completeError } = await supabase
      .from("knowledge_documents")
      .update({
        ingestion_status: "completed",
        ingestion_progress: 100,
      })
      .eq("id", documentId);

    if (completeError) {
      throw new Error(`Failed to mark as completed: ${completeError.message}`);
    }

    console.log(`Completed: ${documentId}`);
  } catch (error) {
    console.error(`Error: ${error.message}`);

    try {
      await supabase
        .from("knowledge_documents")
        .update({
          ingestion_status: "failed",
          last_ingestion_error: error.message,
        })
        .eq("id", documentId);
    } catch (updateError) {
      console.error(
        `Failed to update error status: ${updateError.message}`
      );
    }
  }
}

async function pollDocuments() {
  try {
    const { data: documents, error } = await supabase
      .from("knowledge_documents")
      .select("*")
      .eq("ingestion_status", "pending")
      .limit(1);

    if (error) {
      console.error(`Database error: ${error.message}`);
      return;
    }

    if (documents && documents.length > 0) {
      await processDocument(documents[0]);
    }
  } catch (error) {
    console.error(`Poll error: ${error.message}`);
  }
}

console.log("üöÄ RAG Worker v2 started");
console.log(`üìç Poll interval: ${POLL_INTERVAL}ms`);
console.log(`üì¶ Batch size: ${BATCH_SIZE}`);
console.log(`üìÑ Chunk size: ${CHUNK_SIZE}`);
console.log(`üîë Formats: PDF, DOCX, XLSX, Images (OCR), TXT`);

setInterval(pollDocuments, POLL_INTERVAL);

// Initial poll immediately
pollDocuments();
